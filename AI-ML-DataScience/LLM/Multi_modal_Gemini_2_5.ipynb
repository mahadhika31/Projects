{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "_GDShetSw1MI",
        "outputId": "4e8d35b8-21d0-4ea6-98ee-e475cfa87e61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1-40295609.py:411: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n",
            "  chatbot_component = gr.Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://b27dfd2a3a5dce2a22.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://b27dfd2a3a5dce2a22.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import base64\n",
        "import io\n",
        "import os\n",
        "import time\n",
        "from typing import Dict, List, Optional, Union\n",
        "\n",
        "import gradio as gr\n",
        "from google import genai\n",
        "from google.genai import types  # New types module from google-genai\n",
        "from google.colab import userdata\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "# Retrieve API key for Google GenAI from the environment variables.\n",
        "GOOGLE_API_KEY = userdata.get(\"GEMINI_API_KEY\")\n",
        "\n",
        "# Initialize the client so that it can be reused across functions.\n",
        "CLIENT = genai.Client(api_key=GOOGLE_API_KEY)\n",
        "\n",
        "# General constants for the UI\n",
        "TITLE = \"\"\"<h1 align=\"center\">Gemini 2.5 Multi-modal Chatbot</h1>\"\"\"\n",
        "AVATAR_IMAGES = (None, \"https://media.roboflow.com/spaces/gemini-icon.png\")\n",
        "IMAGE_WIDTH = 512\n",
        "\n",
        "\n",
        "def preprocess_stop_sequences(stop_sequences: str) -> Optional[List[str]]:\n",
        "    \"\"\"\n",
        "    Convert a comma-separated string of stop sequences into a list.\n",
        "\n",
        "    Parameters:\n",
        "        stop_sequences (str): A string containing stop sequences separated by commas.\n",
        "\n",
        "    Returns:\n",
        "        Optional[List[str]]: A list of trimmed stop sequences if provided; otherwise, None.\n",
        "    \"\"\"\n",
        "    if not stop_sequences:\n",
        "        return None\n",
        "    return [sequence.strip() for sequence in stop_sequences.split(\",\")]\n",
        "\n",
        "\n",
        "def preprocess_image(image: Image.Image) -> Image.Image:\n",
        "    \"\"\"\n",
        "    Resize an image to a fixed width while maintaining the aspect ratio.\n",
        "\n",
        "    Parameters:\n",
        "        image (Image.Image): The original image.\n",
        "\n",
        "    Returns:\n",
        "        Image.Image: The resized image with width fixed at IMAGE_WIDTH.\n",
        "    \"\"\"\n",
        "    image_height = int(image.height * IMAGE_WIDTH / image.width)\n",
        "    return image.resize((IMAGE_WIDTH, image_height))\n",
        "\n",
        "\n",
        "def image_to_base64_html_from_pil(image: Image.Image, max_width: int = 150) -> str:\n",
        "    \"\"\"\n",
        "    Convert a PIL Image to an HTML <img> tag with base64-encoded image data.\n",
        "\n",
        "    Parameters:\n",
        "        image (Image.Image): The image to encode.\n",
        "        max_width (int): Maximum width (in pixels) for the displayed image.\n",
        "\n",
        "    Returns:\n",
        "        str: An HTML string with the embedded image.\n",
        "    \"\"\"\n",
        "    buffered = io.BytesIO()\n",
        "    image.save(buffered, format=\"JPEG\")\n",
        "    b64_data = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "    return (\n",
        "        f'<img src=\"data:image/jpeg;base64,{b64_data}\" alt=\"Uploaded Image\" '\n",
        "        f'style=\"max-width:{max_width}px;\">'\n",
        "    )\n",
        "\n",
        "\n",
        "def preprocess_chat_history_messages(\n",
        "    chat_history: List[Union[dict, gr.ChatMessage]],\n",
        ") -> List[Dict[str, Union[str, List[str]]]]:\n",
        "    \"\"\"\n",
        "    Normalize chat history messages into a consistent list of dictionaries.\n",
        "\n",
        "    Each message (whether as a dict or gr.ChatMessage) is converted into a dictionary\n",
        "    containing a role and a list of parts (message content).\n",
        "\n",
        "    Parameters:\n",
        "        chat_history (List[Union[dict, gr.ChatMessage]]): The conversation history.\n",
        "\n",
        "    Returns:\n",
        "        List[Dict[str, Union[str, List[str]]]]: A normalized list of messages.\n",
        "    \"\"\"\n",
        "    messages = []\n",
        "    for msg in chat_history:\n",
        "        if isinstance(msg, dict):\n",
        "            content = msg.get(\"content\")\n",
        "            role = msg.get(\"role\")\n",
        "        else:\n",
        "            content = msg.content\n",
        "            role = msg.role\n",
        "\n",
        "        if content is not None:\n",
        "            # Convert \"assistant\" role to \"model\" if needed.\n",
        "            role = \"model\" if role == \"assistant\" else role\n",
        "            messages.append({\"role\": role, \"parts\": [content]})\n",
        "    return messages\n",
        "\n",
        "\n",
        "def chat_history_to_prompt(chat_history: List[Union[dict, gr.ChatMessage]]) -> str:\n",
        "    \"\"\"\n",
        "    Convert the entire chat conversation into a single text prompt.\n",
        "\n",
        "    Each message is prefixed by “User:” or “Assistant:” to form a full conversation.\n",
        "\n",
        "    Parameters:\n",
        "        chat_history (List[Union[dict, gr.ChatMessage]]): The conversation history.\n",
        "\n",
        "    Returns:\n",
        "        str: A string that concatenates the conversation history.\n",
        "    \"\"\"\n",
        "    conversation = \"\"\n",
        "    for msg in chat_history:\n",
        "        content = get_message_content(msg)\n",
        "        role = msg.get(\"role\") if isinstance(msg, dict) else msg.role\n",
        "        if role in [\"assistant\", \"model\"]:\n",
        "            conversation += f\"Assistant: {content}\\n\"\n",
        "        else:\n",
        "            conversation += f\"User: {content}\\n\"\n",
        "    return conversation\n",
        "\n",
        "\n",
        "def upload(files: Optional[List[str]], chatbot: List[Union[dict, gr.ChatMessage]]):\n",
        "    \"\"\"\n",
        "    Process uploaded image files: resize them, convert to an HTML <img> tag (with base64 data),\n",
        "    and append it as a new user message to the chatbot history.\n",
        "\n",
        "    Parameters:\n",
        "        files (Optional[List[str]]): List of image file paths.\n",
        "        chatbot (List[Union[dict, gr.ChatMessage]]): The current conversation history.\n",
        "\n",
        "    Returns:\n",
        "        List[Union[dict, gr.ChatMessage]]: Updated conversation history.\n",
        "    \"\"\"\n",
        "    for file in files:\n",
        "        image = Image.open(file).convert(\"RGB\")\n",
        "        image = preprocess_image(image)\n",
        "        image_html = image_to_base64_html_from_pil(image)\n",
        "        chatbot.append(gr.ChatMessage(role=\"user\", content=image_html))\n",
        "    return chatbot\n",
        "\n",
        "\n",
        "def upload_audio(\n",
        "    files: Optional[List[str]], chatbot: List[Union[dict, gr.ChatMessage]]\n",
        "):\n",
        "    \"\"\"\n",
        "    Process uploaded audio files: read and base64-encode them, wrap the data in an HTML audio player,\n",
        "    and append it as a new user message.\n",
        "\n",
        "    Parameters:\n",
        "        files (Optional[List[str]]): List of audio file paths.\n",
        "        chatbot (List[Union[dict, gr.ChatMessage]]): The conversation history.\n",
        "\n",
        "    Returns:\n",
        "        List[Union[dict, gr.ChatMessage]]: The updated chatbot history.\n",
        "    \"\"\"\n",
        "    for file in files:\n",
        "        with open(file, \"rb\") as f:\n",
        "            audio_bytes = f.read()\n",
        "        b64_data = base64.b64encode(audio_bytes).decode(\"utf-8\")\n",
        "        audio_html = f\"\"\"<audio controls style=\"max-width:150px;\">\n",
        "  <source src=\"data:audio/mp3;base64,{b64_data}\" type=\"audio/mp3\">\n",
        "  Your browser does not support the audio element.\n",
        "</audio>\"\"\"\n",
        "        chatbot.append(gr.ChatMessage(role=\"user\", content=audio_html))\n",
        "    return chatbot\n",
        "\n",
        "\n",
        "def upload_document(\n",
        "    files: Optional[List[str]], chatbot: List[Union[dict, gr.ChatMessage]]\n",
        "):\n",
        "    \"\"\"\n",
        "    Process uploaded document files (assumed to be PDFs) and add a notification message\n",
        "    (with an HTML snippet) indicating that the document has been uploaded.\n",
        "\n",
        "    Parameters:\n",
        "        files (Optional[List[str]]): List of document file paths.\n",
        "        chatbot (List[Union[dict, gr.ChatMessage]]): The conversation history.\n",
        "\n",
        "    Returns:\n",
        "        List[Union[dict, gr.ChatMessage]]: The updated chatbot history.\n",
        "    \"\"\"\n",
        "    for file in files:\n",
        "        filename = os.path.basename(file)\n",
        "        doc_html = f\"<p>📄 Document uploaded: {filename}</p>\"\n",
        "        chatbot.append(gr.ChatMessage(role=\"user\", content=doc_html))\n",
        "    return chatbot\n",
        "\n",
        "\n",
        "def user(text_prompt: str, chatbot: List[gr.ChatMessage]):\n",
        "    \"\"\"\n",
        "    Append a new user text message to the chat history.\n",
        "\n",
        "    Parameters:\n",
        "        text_prompt (str): The input text provided by the user.\n",
        "        chatbot (List[gr.ChatMessage]): The existing conversation history.\n",
        "\n",
        "    Returns:\n",
        "        Tuple[str, List[gr.ChatMessage]]: A tuple of an empty string (clearing the prompt)\n",
        "            and the updated conversation history.\n",
        "    \"\"\"\n",
        "    if text_prompt:\n",
        "        chatbot.append(gr.ChatMessage(role=\"user\", content=text_prompt))\n",
        "    return \"\", chatbot\n",
        "\n",
        "\n",
        "def get_message_content(msg):\n",
        "    \"\"\"\n",
        "    Retrieve the content of a message that can be either a dictionary or a gr.ChatMessage.\n",
        "\n",
        "    Parameters:\n",
        "        msg (Union[dict, gr.ChatMessage]): The message object.\n",
        "\n",
        "    Returns:\n",
        "        str: The textual content of the message.\n",
        "    \"\"\"\n",
        "    if isinstance(msg, dict):\n",
        "        return msg.get(\"content\", \"\")\n",
        "    return msg.content\n",
        "\n",
        "\n",
        "def bot(\n",
        "    image_files: Optional[List[str]],\n",
        "    audio_files: Optional[List[str]],\n",
        "    doc_files: Optional[List[str]],\n",
        "    chatbot: List[Union[dict, gr.ChatMessage]],\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a chatbot response from Gemini 2.0 based on provided inputs.\n",
        "    This function supports three branches:\n",
        "      1. Document branch: when doc_files are provided.\n",
        "      2. Multi-modal branch: when image and/or audio files are provided.\n",
        "      3. Text-only conversation branch.\n",
        "    All branches now use generate_content_stream to yield incremental responses.\n",
        "\n",
        "    Parameters:\n",
        "        image_files (Optional[List[str]]): List of image file paths.\n",
        "        audio_files (Optional[List[str]]): List of audio file paths.\n",
        "        doc_files (Optional[List[str]]): List of document file paths.\n",
        "        chatbot (List[Union[dict, gr.ChatMessage]]): The conversation history.\n",
        "\n",
        "    Yields:\n",
        "        List[Union[dict, gr.ChatMessage]]: The updated conversation history with streamed responses.\n",
        "    \"\"\"\n",
        "    if len(chatbot) == 0:\n",
        "        return chatbot\n",
        "\n",
        "    # Append a placeholder for the assistant's response.\n",
        "    chatbot.append(gr.ChatMessage(role=\"assistant\", content=\"\"))\n",
        "\n",
        "    generation_config = types.GenerateContentConfig(\n",
        "        temperature=0.4,\n",
        "        max_output_tokens=4096,\n",
        "        top_k=32,\n",
        "        top_p=1,\n",
        "    )\n",
        "\n",
        "    # Branch 1: Document uploads.\n",
        "    if doc_files and len(doc_files) > 0:\n",
        "        prev_msg_content = get_message_content(chatbot[-2]) if len(chatbot) >= 2 else \"\"\n",
        "        prompt = [prev_msg_content] if prev_msg_content else []\n",
        "        doc_parts = []\n",
        "        for file in doc_files:\n",
        "            with open(file, \"rb\") as f:\n",
        "                doc_bytes = f.read()\n",
        "            doc_parts.append(\n",
        "                types.Part.from_bytes(\n",
        "                    data=doc_bytes,\n",
        "                    mime_type=\"application/pdf\",\n",
        "                )\n",
        "            )\n",
        "        # Combine document parts and previous text.\n",
        "        contents = doc_parts + prompt\n",
        "        # Use the streaming endpoint.\n",
        "        response = CLIENT.models.generate_content_stream(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            contents=contents,\n",
        "            config=generation_config,\n",
        "        )\n",
        "        for chunk in response:\n",
        "            for i in range(0, len(chunk.text), 10):\n",
        "                section = chunk.text[i : i + 10]\n",
        "                if isinstance(chatbot[-1], dict):\n",
        "                    chatbot[-1][\"content\"] += section\n",
        "                else:\n",
        "                    chatbot[-1].content += section\n",
        "                time.sleep(0.01)\n",
        "                yield chatbot\n",
        "        return\n",
        "\n",
        "    # Branch 2: Image or audio uploads.\n",
        "    elif (image_files and len(image_files) > 0) or (\n",
        "        audio_files and len(audio_files) > 0\n",
        "    ):\n",
        "        prev_msg_content = get_message_content(chatbot[-2]) if len(chatbot) >= 2 else \"\"\n",
        "        text_prompt = [prev_msg_content] if prev_msg_content else []\n",
        "        image_prompt = (\n",
        "            [Image.open(file).convert(\"RGB\") for file in image_files]\n",
        "            if image_files\n",
        "            else []\n",
        "        )\n",
        "        audio_prompt = []\n",
        "        if audio_files:\n",
        "            for file in audio_files:\n",
        "                with open(file, \"rb\") as f:\n",
        "                    audio_bytes = f.read()\n",
        "                audio_prompt.append(\n",
        "                    types.Part.from_bytes(\n",
        "                        data=audio_bytes,\n",
        "                        mime_type=\"audio/mp3\",\n",
        "                    )\n",
        "                )\n",
        "        # Combine all inputs into a multi-modal prompt.\n",
        "        contents = text_prompt + image_prompt + audio_prompt\n",
        "        response = CLIENT.models.generate_content_stream(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            contents=contents,\n",
        "            config=generation_config,\n",
        "        )\n",
        "        for chunk in response:\n",
        "            for i in range(0, len(chunk.text), 10):\n",
        "                section = chunk.text[i : i + 10]\n",
        "                if isinstance(chatbot[-1], dict):\n",
        "                    chatbot[-1][\"content\"] += section\n",
        "                else:\n",
        "                    chatbot[-1].content += section\n",
        "                time.sleep(0.01)\n",
        "                yield chatbot\n",
        "        return\n",
        "\n",
        "    # Branch 3: Text-only conversation.\n",
        "    else:\n",
        "        conversation_text = chat_history_to_prompt(chatbot)\n",
        "        response = CLIENT.models.generate_content_stream(\n",
        "            model=\"gemini-2.5-flash\",\n",
        "            contents=[conversation_text],\n",
        "            config=generation_config,\n",
        "        )\n",
        "        for chunk in response:\n",
        "            for i in range(0, len(chunk.text), 10):\n",
        "                section = chunk.text[i : i + 10]\n",
        "                if isinstance(chatbot[-1], dict):\n",
        "                    chatbot[-1][\"content\"] += section\n",
        "                else:\n",
        "                    chatbot[-1].content += section\n",
        "                time.sleep(0.01)\n",
        "                yield chatbot\n",
        "        return\n",
        "\n",
        "\n",
        "def run_code_execution(code_prompt: str, chatbot: List[Union[dict, gr.ChatMessage]]):\n",
        "    \"\"\"\n",
        "    Append the user's code execution query to the chat history, then call Gemini\n",
        "    with code execution enabled using the user's input. The results (including any\n",
        "    generated code and execution output) are appended as a new assistant message.\n",
        "    \"\"\"\n",
        "    # Only add a user message if there is content.\n",
        "    if code_prompt.strip():\n",
        "        chatbot.append(gr.ChatMessage(role=\"user\", content=code_prompt))\n",
        "    # Append an empty assistant message to update with the code execution response.\n",
        "    chatbot.append(gr.ChatMessage(role=\"assistant\", content=\"\"))\n",
        "\n",
        "    generation_config = types.GenerateContentConfig(\n",
        "        tools=[types.Tool(code_execution=types.ToolCodeExecution)]\n",
        "    )\n",
        "    response = CLIENT.models.generate_content(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        contents=code_prompt,\n",
        "        config=generation_config,\n",
        "    )\n",
        "\n",
        "    output_text = \"\"\n",
        "    for part in response.candidates[0].content.parts:\n",
        "        if part.text is not None:\n",
        "            output_text += f\"{part.text}\\n\"\n",
        "        if part.executable_code is not None:\n",
        "            # Display the executable code in a code block (using markdown formatting)\n",
        "            output_text += (\n",
        "                f\"\\n**Generated Code:**\\n```python\\n{part.executable_code.code}\\n```\\n\"\n",
        "            )\n",
        "        if part.code_execution_result is not None:\n",
        "            output_text += (\n",
        "                f\"\\n**Output:**\\n```\\n{part.code_execution_result.output}\\n```\\n\"\n",
        "            )\n",
        "        if part.inline_data is not None:\n",
        "            image_data = base64.b64decode(part.inline_data.data)\n",
        "            image = Image.open(io.BytesIO(image_data))\n",
        "            buffered = io.BytesIO()\n",
        "            image.save(buffered, format=\"PNG\")\n",
        "            b64_data = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
        "            output_text += f'\\n<img src=\"data:image/png;base64,{b64_data}\" alt=\"Inline Image\" style=\"max-width:300px;\"/>\\n'\n",
        "        output_text += \"\\n---\\n\"\n",
        "\n",
        "    # Update the last assistant message with the code execution result.\n",
        "    if isinstance(chatbot[-1], dict):\n",
        "        chatbot[-1][\"content\"] = output_text\n",
        "    else:\n",
        "        chatbot[-1].content = output_text\n",
        "\n",
        "    # Clear the text prompt after processing.\n",
        "    return \"\", chatbot\n",
        "\n",
        "\n",
        "# Define the Gradio UI components.\n",
        "chatbot_component = gr.Chatbot(\n",
        "    label=\"Gemini 2.5\",\n",
        "    type=\"messages\",  # Using message objects.\n",
        "    bubble_full_width=False,\n",
        "    avatar_images=AVATAR_IMAGES,\n",
        "    scale=2,\n",
        "    height=400,\n",
        ")\n",
        "text_prompt_component = gr.Textbox(\n",
        "    placeholder=\"Enter your message or code query here...\",\n",
        "    show_label=False,\n",
        "    autofocus=True,\n",
        "    scale=19,\n",
        ")\n",
        "upload_button_component = gr.UploadButton(\n",
        "    label=\"Upload Images\",\n",
        "    file_count=\"multiple\",\n",
        "    file_types=[\"image\"],\n",
        "    scale=1,\n",
        ")\n",
        "upload_audio_button_component = gr.UploadButton(\n",
        "    label=\"Upload Audio\",\n",
        "    file_count=\"multiple\",\n",
        "    file_types=[\"audio\"],\n",
        "    scale=1,\n",
        ")\n",
        "upload_doc_button_component = gr.UploadButton(\n",
        "    label=\"Upload Documents\",\n",
        "    file_count=\"multiple\",\n",
        "    file_types=[\".pdf\"],\n",
        "    scale=1,\n",
        ")\n",
        "run_button_component = gr.Button(value=\"Run\", variant=\"primary\", scale=1, min_width=60)\n",
        "run_code_execution_button = gr.Button(\n",
        "    value=\"Run Code Execution\", variant=\"secondary\", scale=1\n",
        ")\n",
        "\n",
        "# Define input lists for button chaining.\n",
        "user_inputs = [text_prompt_component, chatbot_component]\n",
        "bot_inputs = [\n",
        "    upload_button_component,\n",
        "    upload_audio_button_component,\n",
        "    upload_doc_button_component,\n",
        "    chatbot_component,\n",
        "]\n",
        "\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.HTML(TITLE)\n",
        "    with gr.Column():\n",
        "        chatbot_component.render()\n",
        "        with gr.Row(equal_height=True):\n",
        "            text_prompt_component.render()\n",
        "            run_button_component.render()\n",
        "        with gr.Row():\n",
        "            # Render file-upload buttons and the code execution button in a single row.\n",
        "            upload_button_component.render()\n",
        "            upload_audio_button_component.render()\n",
        "            upload_doc_button_component.render()\n",
        "            run_code_execution_button.render()\n",
        "\n",
        "    # When the Run button is clicked, first process the user text then stream a response.\n",
        "    run_button_component.click(\n",
        "        fn=user,\n",
        "        inputs=user_inputs,\n",
        "        outputs=[text_prompt_component, chatbot_component],\n",
        "        queue=False,\n",
        "    ).then(\n",
        "        fn=bot,\n",
        "        inputs=bot_inputs,\n",
        "        outputs=[chatbot_component],\n",
        "    )\n",
        "\n",
        "    # Allow submission using the Enter key.\n",
        "    text_prompt_component.submit(\n",
        "        fn=user,\n",
        "        inputs=user_inputs,\n",
        "        outputs=[text_prompt_component, chatbot_component],\n",
        "        queue=False,\n",
        "    ).then(\n",
        "        fn=bot,\n",
        "        inputs=bot_inputs,\n",
        "        outputs=[chatbot_component],\n",
        "    )\n",
        "\n",
        "    # Handle image uploads.\n",
        "    upload_button_component.upload(\n",
        "        fn=upload,\n",
        "        inputs=[upload_button_component, chatbot_component],\n",
        "        outputs=[chatbot_component],\n",
        "        queue=False,\n",
        "    )\n",
        "\n",
        "    # Handle audio uploads.\n",
        "    upload_audio_button_component.upload(\n",
        "        fn=upload_audio,\n",
        "        inputs=[upload_audio_button_component, chatbot_component],\n",
        "        outputs=[chatbot_component],\n",
        "        queue=False,\n",
        "    )\n",
        "\n",
        "    # Handle document uploads.\n",
        "    upload_doc_button_component.upload(\n",
        "        fn=upload_document,\n",
        "        inputs=[upload_doc_button_component, chatbot_component],\n",
        "        outputs=[chatbot_component],\n",
        "        queue=False,\n",
        "    )\n",
        "\n",
        "    # When the Code Execution button is clicked, process the code prompt and stream the output.\n",
        "    run_code_execution_button.click(\n",
        "        fn=run_code_execution,\n",
        "        inputs=[text_prompt_component, chatbot_component],\n",
        "        outputs=[text_prompt_component, chatbot_component],\n",
        "        queue=False,\n",
        "    )\n",
        "\n",
        "# Launch the demo interface with queuing enabled.\n",
        "demo.queue(max_size=99, api_open=False).launch(debug=False, pwa=True, show_error=True)"
      ]
    }
  ]
}